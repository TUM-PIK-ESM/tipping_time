---
title: "Applying the Methods of DD23 to data from alternative models"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    code_folding: show
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE, 
                      class.source = 'fold-hide')

```

Adopting the code supplementing the results of DD23, we analyse how robust their methods are with respect to data from alternative synthetic models.

The code is written in R:

R Core Team (2022). R: A language and environment for statistical computing. R Foundation for
  Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.


## Preliminaries

The following code chunk loads the required R packages.

```{r }

library(ggplot2)   ## For nice plotting
library(stats4)    ## For maximum likelihood facilities (mle())
library(writexl)   ## For writing data frames into excel files
library(readxl)    ## For reading data frames into excel files
library(stringr)   ## For manipulating strings
```

The following chunk code defines functions for likelihood calculations and simulations. Taken from DD23 without changes.

```{r }
## Function returning 2 x the negative log-likelihood of a trace up to a constant
## The trace is assumed an Ornstein-Uhlenbeck with constant mean mu and rate
## parameter alpha. 
loglikOU.fun = function(pars, data, delta){
  ## pars = (alpha, mu, sigma) are the parameters to be estimated in the model:
  ## dXt = - alpha * ( Xt - mu ) dt + sigma * dWt
  ## data is a trace 
  ## delta is the time step between observations
  alpha0 = max(pars[1],0.001) #Ensuring alpha is positive
  mu0    = pars[2]            #Mean
  sigma2 = max(0,pars[3])     #Infinitisimal variance, should be positive
  n      = length(data)
  Xupp   = data[2:n]
  Xlow   = data[1:(n-1)]
  time   = delta*(1:(n-1))
  gamma2 = sigma2/(2*alpha0)  #Asymptotic variance
  rho0   = exp(-alpha0*delta) #Autocorrelation
  m.part = Xupp - Xlow*rho0 - mu0*(1-rho0) #Observation minus the mean of transition distribution
  v.part = gamma2*(1-rho0^2) #Variance of transition distribution
  loglik = - n*(log(v.part)) - sum(m.part^2/v.part)
  -loglik
}

## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations
estimate.OU = function(data, delta, initial.values = NULL){
  if(is.null(initial.values)){
    n    = length(data)
    Xupp = data[2:n]
    Xlow = data[1:(n-1)]
    mu   = mean(data) ##Starting value for mu0 
    alpha = -log(sum((Xupp-mu)*(Xlow-mu))/sum((Xlow-mu)^2))/delta ##Starting value for alpha0 
    s2 = mean(diff(data)^2)/delta ##Quadratic variation, starting value for sigma^2
    par.init = c(alpha, mu, s2)   ## Collect starting values
  }
  if(!is.null(initial.values)) par.init = initial.values
  minuslogl = function(alpha,mu,sigma2){
    logLik = tryCatch(loglikOU.fun(pars = c(alpha,mu,sigma2), data = data, delta = delta))
    if(is.na(logLik))
      {
        return(10000)
      }else
      {
        return(logLik)
      }
    }
  temp = stats4::mle(minuslogl = minuslogl, 
             start = list(alpha = par.init[1], mu = par.init[2], sigma2 = par.init[3]))
  return(temp)
}

## Function returning 2 x the negative log-likelihood of a trace up to a constant from model
## dXt = - (a*(Xt - m)^2 + lambda) dt + sigma * dWt
## Assuming alpha0, mu0, sigma known
## Strang splitting estimator based on Taylor expansion around mu(lambda)
loglik.fun = function(pars, data, delta, alpha0, mu0, sigma20, pen = 0){
  ##pars are the parameters to be estimated, tau and a
  ##data is a trace under linear changing of lambda
  ##delta is the time step between observations
  ##alpha0, mu0, sigma20 are parameters already estimated in the OU process from stationary part
  tau     = pars[1]             #Ramping time
  a       = max(0.1,pars[2])    #Factor in front of (x-m)^2 in drift. Positive
  m       = mu0 - alpha0/(2*a)  #Constant mean shift
  lambda0 = -alpha0^2/(4*a)     #Stationary level of control parameter
  sigma2  = sigma20             #Infinitisimal variance
  n       = length(data)
  Xupp    = data[2:n]
  Xlow    = data[1:(n-1)]
  time    = delta*(1:(n-1))
  lam.seq    = lambda0*(1-time/tau)
  alpha.seq  = 2*sqrt(-a*lam.seq)
  gamma2.seq = sigma2/(2*alpha.seq)
  rho.seq    = exp(-alpha.seq*delta)
  mu.seq     = m + sqrt(-lam.seq/a)
  ## Calculating the Strang splitting scheme pseudo likelihood
    fh.half.tmp = a*delta*(Xlow - mu.seq)/2
    fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
    fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
    mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
    m.part      = fh.half.inv - mu.h
    var.part    = gamma2.seq*(1-rho.seq^2)
    det.Dfh.half.inv = 1/(a*delta*(Xupp-mu.seq)/2-1)^2
    loglik      = - sum(log(var.part)) - sum(m.part^2/var.part) + 
      2*sum(log(det.Dfh.half.inv)) - pen*n*(1/a - 1)*(a < 1)
  return(-loglik)
}

## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations, and time passed
## at present since time t_0
estimate.tipping = function(data, delta, initial.values = c(100,1),  
                            alpha0, mu0, sigma20, pen = pen){
  par.init = initial.values
  minuslogl = function(pars){
    logLik = tryCatch(loglik.fun(pars = pars, data = data, delta = delta,
               alpha0 = alpha0, mu0 = mu0, sigma20 = sigma20, pen = pen))
    if(is.na(logLik))
      {
        return(10000)
      }else
      {
        return(logLik)
      }
    }
  temp = optim(par = c(tau = par.init[1], a = par.init[2]), 
               fn = minuslogl, method = "Nelder-Mead", hessian = FALSE)
  return(temp)
}

## Function returning a trajectory up to a crossing time of X(t)  
## over the barrier as a function of
## the initial condition X0, the size of the noise sigma, 
## the integration time step dt, and the length of the simulation N
## and with time varying lambda:
X.traj <- function(sigma = 0.1, lambda0 = -2, tau = 1000, m = 0, a = 1,  
                   T0 = 0, X0 = sqrt(2), dt = 0.1, Ymax = 1000000){
  ##T0: Time before ramping starts
  ##Ymax: Max number of simulated points, if tipping does not happen
  Y = 0  ## Counting integration steps up to tipping
  xbarrier = m - 2 ## One smaller than crossing point at start
  Xtraj = X0
  X = X0
  ## Simulation during stationary period, constant lambda
  while(X > xbarrier & Y < T0/dt){
    X = S.onestep(sigma = sigma, lambda = lambda0,  
                  m = m, a = a, X0 = X, dt = dt)
    Xtraj = c(Xtraj, X)
    Y = Y+1
  }
  ## Simulation after lambda starts increasing
  while(X > xbarrier & Y < Ymax){
    time = dt * (Y - T0/dt) ## Time after lambda starts increasing
    lambda = lambda0*(1-time/tau)
    X = S.onestep(sigma = sigma, lambda = lambda0*(1 - time/tau), 
                  m = m, a = a, X0 = X, dt = dt)
    Xtraj = c(Xtraj, X)
    Y = Y + 1
  }
  Y = Y*dt
  return(list(FPT = Y, X = Xtraj))
}

## Function returning a simulation of the simple model one time step
## using the Euler-Maruyama scheme
S.onestep <- function(sigma = 0.1, lambda = 0, m = 0, a = 1, X0 = 1, dt = 0.1){
  dWt = rnorm(1,0,1)    ## Draws a random increment
  Y   = X0 - a*(X0-m)^2*dt - lambda*dt + sqrt(dt)*sigma*dWt
  return(Y)
}

```

## Estimating tipping time on synthetic model data

Time series of different synthetic models are read from txt files generated in the accompanying ipynb file. On each time series, the parameter estimation routine introduced by DD23 is applied and the results are written into xlsx files. Plots are generated in the aforementioned ipynb file.

```{r }

## Take the estimates from the AMOC data
t0 = 1924 ## Knowledge of t0 is presupposed. All synthetic models have the same true value of t0 = 1924.
#alpha0 = 3.06
#mu0 = 0.25
#sigma2 = 0.3/9
#tau = 132.52
#a = 0.87
#m = -1.51
#lambda0 = -2.69
#tc = 2056.52

##############################
## Estimation on each trace ##
##############################


for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
#for (filepath in c("SamplePathData/fnf_wn_linforc.csv")){
  modelname = str_sub(filepath,16,-5) 
  print(modelname)
  n = round((2021-(1870+1/12))*12+1)
  Delta_t = 1/12

  xx = data.matrix(read.csv(filepath))[,-1]
  nsim = dim(xx)[2]   ## Number of simulations
  xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
  xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
                       repetition = rep(1:nsim, each = n))
  ## xx.data now contains nsim time series of the current examined model
  
  
  pen = 0   ## First with no penalization, determine the estimated tau_r
  estim.alpha0 = numeric(nsim)
  estim.mu0 = numeric(nsim)
  estim.s2 = numeric(nsim)
  estim.tau = numeric(nsim)
  estim.a = numeric(nsim)
  for(isim in 1:nsim){
    ## Get data from isim simulation after time t0, when linear ramping has started
    xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
    xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
    ## Get baseline data from isim simulation, before time t0
    xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
    nx = length(xxi)

    temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
    estim.alpha0[isim] = coef(temp1)[1]
    estim.mu0[isim]   = coef(temp1)[2]
    estim.s2[isim]     = coef(temp1)[3]

    temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1),
              alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
    estim.tau[isim] = temp2$par[1]
    estim.a[isim]  = temp2$par[2]
  }
  alpha0_pen0 = median(estim.alpha0)
  mu0_pen0 = median(estim.mu0)
  s2_pen0 = median(estim.s2)
  tau_pen0 = median(estim.tau) ## This value is the fitting target for the subsequent search of an optimal pen
  a_pen0 = median(estim.a)
  m_pen0 = median(estim.mu0 - estim.alpha0/(2*estim.a))
  lambda0_pen0 = median(- estim.alpha0^2/(4*estim.a))
  tc_pen0 = median(estim.tau + t0)

  #print(paste("alpha0_pen0:",alpha0_pen0))
  #print(paste("mu0_pen0:",mu0_pen0))
  #print(paste("s2_pen0:",s2_pen0))
  #print(paste("tau_pen0:",tau_pen0))
  #print(paste("a_pen0:",a_pen0))
  #print(paste("m_pen0:",m_pen0))
  #print(paste("lambda0_pen0:",lambda0_pen0))
  #print(paste("tc_pen0:",tc_pen0))

  #Define parameters
  crossval_nsim = floor(nsim/10) ## Number of cross validation time series
  T0      = t0-1870      ## Time length of stationary dynamics (lambda constant)
  sig     = sqrt(s2_pen0) ## Infinitisimal standard deviation: sigma
  time    = seq(from = 1870, to = 2020, length.out=n)
  lam.seq = lambda0_pen0*(1-pmax(0,(time-t0))/tau_pen0) ## Time varying lambda
  alpha.seq = 2*sqrt(-a_pen0*lam.seq) ## Time varying alpha
  mu.seq  = m_pen0 + sqrt(-lam.seq/a_pen0) ## Time varying mu
  var0    = s2_pen0/(2*alpha0_pen0)    ## Time varying variance
  Delta_t = 1/12                 ## Time step in years (one measurement per month)
  nloop   = 20                   ## Number of simulation steps between observation steps
  tp      = 2020 - t0            ## Time length of non-stationary dynamics (lambda linearly increasing)
  nt      = ceiling(tp/Delta_t) ## Total number of points from t_0 to today
  ## Generate synthetic time series to find the optimal pen on
  crossval_xx = matrix(NA, n, crossval_nsim)

  for (isim in 1:crossval_nsim){
    #Initial condition is random from stationary distribution
    x0 = m_pen0 + sqrt(-lambda0_pen0/a_pen0) + rnorm(1, mean = 0, sd = sig/sqrt(2*alpha0_pen0))

    xxx = X.traj(sigma = sig, lambda0 = lambda0_pen0, tau = tau_pen0, m = m_pen0, a = a_pen0,
                 T0 = T0, X0 = x0, dt = Delta_t/nloop,
                 Ymax = nloop*n-1)
    nx = length(xxx$X) ##If tipping happens before, trajectory is shorter than nt
    xxxx = xxx$X[seq(1, nx, nloop)] ##Only keep points at observed times
    nxx = length(xxxx)
    crossval_xx[1:nxx, isim] = xxxx
  }

  xxtime <- time[1:n]
  crossval_xx.data = data.frame(X = c(crossval_xx), time = rep(xxtime, crossval_nsim),
                       repetition = rep(1:crossval_nsim, each = n),
                       model = rep(m_pen0 + sqrt(-lam.seq[1:n]/a_pen0), crossval_nsim))

  ## End of simulation of crossval_nsim paths


  p.vec = seq(0,0.01,0.001) ## Try this range of values for pen. For the AMOC time series in DD23, 0.004 was the optimal value.
  MSE = matrix(0,ncol = 2, nrow = length(p.vec))
  dimnames(MSE)[[2]] = c("p","MSE")
  MSE[,1] = p.vec
  for(i in 1:length(p.vec)){ ## For every choice of pen, determine the distribution of estimated tau_r
    pen = p.vec[i]
    estim.tau = numeric(crossval_nsim) ## Vector to hold estimates

    for(isim in 1:crossval_nsim){
      xxi   = crossval_xx.data[crossval_xx.data$time > t0 & crossval_xx.data$repetition == isim,"X"]  ##Get data from isim simulation
      xxi.0 = crossval_xx.data[crossval_xx.data$time <= t0 & crossval_xx.data$repetition == isim,"X"] ##Get data from isim simulation
      temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
      temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1),
                alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
      estim.tau[isim] = temp2$par["tau"]
    }
    MSE[i,2] = mean((estim.tau-tau_pen0)^2) ## The distributions are assessed in their square error w.r.t. the fitting target at pen = 0
  }



  ## Do full tipping time analysis on all time series with optimal value of penalization p
  pen = p.vec[which.min(c(MSE[,2]))] ## pen is chosen as the value where the above square error was minimal
  print(paste("Optimal pen:",pen))
  estim.matrix = matrix(0, ncol = 8, nrow = nsim) ## Matrix to hold estimates
  dimnames(estim.matrix)[[2]] = c("alpha0","mu0","lambda0", "tau", "s2", "m", "a", "tc")
  
  for(isim in 1:nsim){
    ## Get data from isim simulation after time t0, when linear ramping has started
    xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]  
    xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
    ## Get baseline data from isim simulation, before time t0
    xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"] 
    nx = length(xxi)
    
    temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
    estim.matrix[isim,"alpha0"] = coef(temp1)[1]
    estim.matrix[isim,"mu0"]    = coef(temp1)[2]
    estim.matrix[isim,"s2"]     = coef(temp1)[3]
  
    temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1), 
              alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
    estim.matrix[isim,"tau"] = temp2$par[1]
    estim.matrix[isim, "a"]  = temp2$par[2]
  }
  
  estim.matrix[,"m"] = estim.matrix[,"mu0"] - estim.matrix[,"alpha0"]/(2*estim.matrix[,"a"])
  estim.matrix[,"lambda0"] = - estim.matrix[,"alpha0"]^2/(4*estim.matrix[,"a"])
  estim.matrix[,"tc"] = estim.matrix[,"tau"] + t0
  
  
  write_xlsx(as.data.frame(estim.matrix), paste("EstimMatrix_",modelname,".xlsx",sep=""))
}

```

## Checking the model with normal residuals

Here, residuals are calculated and plotted in a qq-plot, similar to Figure 6 in the paper. The qq-plot shows a good fit to the model of DD23.

```{r, eval = TRUE }
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
  modelname = str_sub(filepath,16,-5)

  take_sample = 2
  n = round((2021-(1870+1/12))*12+1)
  Delta_t = 1/12
  
  estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
  
  ## read the desired sample path
  xx = data.matrix(read.csv(filepath))[,-1]
  xxtime = seq(from = 1870, to = 2020, length.out=n)
  xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
                       repetition = rep(1:nsim, each = n))
  
  ## Generate the qq-plot, directly adopted from DD23.
  n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
  Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
  Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
  time       = Delta_t*(1:(n-1))
  lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
  alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
  mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
  gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
  rho.seq    = exp(-alpha.seq*Delta_t)
  
  mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
  mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
  mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
  sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
  
  fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
  fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
  fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
  mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
  m.part      = fh.half.inv - mu.h
  var.part    = gamma2.seq*(1-rho.seq^2)
  norm.resid.S = m.part/sd.seq
  write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
  qqnorm(norm.resid.S, main = modelname)
  qqline(norm.resid.S)
}

```

