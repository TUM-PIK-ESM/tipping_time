-loglik
}
## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations
estimate.OU = function(data, delta, initial.values = NULL){
if(is.null(initial.values)){
n    = length(data)
Xupp = data[2:n]
Xlow = data[1:(n-1)]
mu   = mean(data) ##Starting value for mu0
alpha = -log(sum((Xupp-mu)*(Xlow-mu))/sum((Xlow-mu)^2))/delta ##Starting value for alpha0
s2 = mean(diff(data)^2)/delta ##Quadratic variation, starting value for sigma^2
par.init = c(alpha, mu, s2)   ## Collect starting values
}
if(!is.null(initial.values)) par.init = initial.values
minuslogl = function(alpha,mu,sigma2){
logLik = tryCatch(loglikOU.fun(pars = c(alpha,mu,sigma2), data = data, delta = delta))
if(is.na(logLik))
{
return(10000)
}else
{
return(logLik)
}
}
temp = stats4::mle(minuslogl = minuslogl,
start = list(alpha = par.init[1], mu = par.init[2], sigma2 = par.init[3]))
return(temp)
}
## Function returning 2 x the negative log-likelihood of a trace up to a constant from model
## dXt = - (a*(Xt - m)^2 + lambda) dt + sigma * dWt
## Assuming alpha0, mu0, sigma known
## Strang splitting estimator based on Taylor expansion around mu(lambda)
loglik.fun = function(pars, data, delta, alpha0, mu0, sigma20, pen = 0){
##pars are the parameters to be estimated, tau and a
##data is a trace under linear changing of lambda
##delta is the time step between observations
##alpha0, mu0, sigma20 are parameters already estimated in the OU process from stationary part
tau     = pars[1]             #Ramping time
a       = max(0.1,pars[2])    #Factor in front of (x-m)^2 in drift. Positive
m       = mu0 - alpha0/(2*a)  #Constant mean shift
lambda0 = -alpha0^2/(4*a)     #Stationary level of control parameter
sigma2  = sigma20             #Infinitisimal variance
n       = length(data)
Xupp    = data[2:n]
Xlow    = data[1:(n-1)]
time    = delta*(1:(n-1))
lam.seq    = lambda0*(1-time/tau)
alpha.seq  = 2*sqrt(-a*lam.seq)
gamma2.seq = sigma2/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*delta)
mu.seq     = m + sqrt(-lam.seq/a)
## Calculating the Strang splitting scheme pseudo likelihood
fh.half.tmp = a*delta*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
det.Dfh.half.inv = 1/(a*delta*(Xupp-mu.seq)/2-1)^2
loglik      = - sum(log(var.part)) - sum(m.part^2/var.part) +
2*sum(log(det.Dfh.half.inv)) - pen*n*(1/a - 1)*(a < 1)
return(-loglik)
}
## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations, and time passed
## at present since time t_0
estimate.tipping = function(data, delta, initial.values = c(100,1),
alpha0, mu0, sigma20, pen = pen){
par.init = initial.values
minuslogl = function(pars){
logLik = tryCatch(loglik.fun(pars = pars, data = data, delta = delta,
alpha0 = alpha0, mu0 = mu0, sigma20 = sigma20, pen = pen))
if(is.na(logLik))
{
return(10000)
}else
{
return(logLik)
}
}
temp = optim(par = c(tau = par.init[1], a = par.init[2]),
fn = minuslogl, method = "Nelder-Mead", hessian = FALSE)
return(temp)
}
## Function returning a trajectory up to a crossing time of X(t)
## over the barrier as a function of
## the initial condition X0, the size of the noise sigma,
## the integration time step dt, and the length of the simulation N
## and with time varying lambda:
X.traj <- function(sigma = 0.1, lambda0 = -2, tau = 1000, m = 0, a = 1,
T0 = 0, X0 = sqrt(2), dt = 0.1, Ymax = 1000000){
##T0: Time before ramping starts
##Ymax: Max number of simulated points, if tipping does not happen
Y = 0  ## Counting integration steps up to tipping
xbarrier = m - 2 ## One smaller than crossing point at start
Xtraj = X0
X = X0
## Simulation during stationary period, constant lambda
while(X > xbarrier & Y < T0/dt){
X = S.onestep(sigma = sigma, lambda = lambda0,
m = m, a = a, X0 = X, dt = dt)
Xtraj = c(Xtraj, X)
Y = Y+1
}
## Simulation after lambda starts increasing
while(X > xbarrier & Y < Ymax){
time = dt * (Y - T0/dt) ## Time after lambda starts increasing
lambda = lambda0*(1-time/tau)
X = S.onestep(sigma = sigma, lambda = lambda0*(1 - time/tau),
m = m, a = a, X0 = X, dt = dt)
Xtraj = c(Xtraj, X)
Y = Y + 1
}
Y = Y*dt
return(list(FPT = Y, X = Xtraj))
}
## Function returning a simulation of the simple model one time step
## using the Euler-Maruyama scheme
S.onestep <- function(sigma = 0.1, lambda = 0, m = 0, a = 1, X0 = 1, dt = 0.1){
dWt = rnorm(1,0,1)    ## Draws a random increment
Y   = X0 - a*(X0-m)^2*dt - lambda*dt + sqrt(dt)*sigma*dWt
return(Y)
}
## Take the estimates from the AMOC data
t0 = 1924 ## Knowledge of t0 is presupposed. All synthetic models have the same true value of t0 = 1924.
#alpha0 = 3.06
#mu0 = 0.25
#sigma2 = 0.3/9
#tau = 132.52
#a = 0.87
#m = -1.51
#lambda0 = -2.69
#tc = 2056.52
##############################
## Estimation on each trace ##
##############################
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
#for (filepath in c("SamplePathData/ou_rn_linforc.csv")){
modelname = str_sub(filepath,16,-5)
print(modelname)
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
xx = data.matrix(read.csv(filepath))[,-1]
nsim = dim(xx)[2]   ## Number of simulations
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## xx.data now contains nsim time series of the current examined model
pen = 0
run_penalization_routine = TRUE ## If TRUE the optimal pen as the value with least spread around the tc estimation corresponding to pen = 0
if (run_penalization_routine){
## First with no penalization, determine the estimated tau_r
estim.alpha0 = numeric(nsim)
estim.mu0 = numeric(nsim)
estim.s2 = numeric(nsim)
estim.tau = numeric(nsim)
estim.a = numeric(nsim)
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
estim.alpha0[isim] = coef(temp1)[1]
estim.mu0[isim]   = coef(temp1)[2]
estim.s2[isim]     = coef(temp1)[3]
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.tau[isim] = temp2$par[1]
estim.a[isim]  = temp2$par[2]
}
alpha0_pen0 = median(estim.alpha0)
mu0_pen0 = median(estim.mu0)
s2_pen0 = median(estim.s2)
tau_pen0 = median(estim.tau) ## This value is the fitting target for the subsequent search of an optimal pen
a_pen0 = median(estim.a)
m_pen0 = median(estim.mu0 - estim.alpha0/(2*estim.a))
lambda0_pen0 = median(- estim.alpha0^2/(4*estim.a))
tc_pen0 = median(estim.tau + t0)
#print(paste("alpha0_pen0:",alpha0_pen0))
#print(paste("mu0_pen0:",mu0_pen0))
#print(paste("s2_pen0:",s2_pen0))
#print(paste("tau_pen0:",tau_pen0))
#print(paste("a_pen0:",a_pen0))
#print(paste("m_pen0:",m_pen0))
#print(paste("lambda0_pen0:",lambda0_pen0))
#print(paste("tc_pen0:",tc_pen0))
#Define parameters
crossval_nsim = 50 ## Number of cross validation time series
T0      = t0-1870      ## Time length of stationary dynamics (lambda constant)
sig     = sqrt(s2_pen0) ## Infinitisimal standard deviation: sigma
time    = seq(from = 1870, to = 2020, length.out=n)
lam.seq = lambda0_pen0*(1-pmax(0,(time-t0))/tau_pen0) ## Time varying lambda
alpha.seq = 2*sqrt(-a_pen0*lam.seq) ## Time varying alpha
mu.seq  = m_pen0 + sqrt(-lam.seq/a_pen0) ## Time varying mu
var0    = s2_pen0/(2*alpha0_pen0)    ## Time varying variance
Delta_t = 1/12                 ## Time step in years (one measurement per month)
nloop   = 20                   ## Number of simulation steps between observation steps
tp      = 2020 - t0            ## Time length of non-stationary dynamics (lambda linearly increasing)
nt      = ceiling(tp/Delta_t) ## Total number of points from t_0 to today
## Generate synthetic time series to find the optimal pen on
crossval_xx = matrix(NA, n, crossval_nsim)
for (isim in 1:crossval_nsim){
#Initial condition is random from stationary distribution
x0 = m_pen0 + sqrt(-lambda0_pen0/a_pen0) + rnorm(1, mean = 0, sd = sig/sqrt(2*alpha0_pen0))
xxx = X.traj(sigma = sig, lambda0 = lambda0_pen0, tau = tau_pen0, m = m_pen0, a = a_pen0,
T0 = T0, X0 = x0, dt = Delta_t/nloop,
Ymax = nloop*n-1)
nx = length(xxx$X) ##If tipping happens before, trajectory is shorter than nt
xxxx = xxx$X[seq(1, nx, nloop)] ##Only keep points at observed times
nxx = length(xxxx)
crossval_xx[1:nxx, isim] = xxxx
}
xxtime <- time[1:n]
crossval_xx.data = data.frame(X = c(crossval_xx), time = rep(xxtime, crossval_nsim),
repetition = rep(1:crossval_nsim, each = n),
model = rep(m_pen0 + sqrt(-lam.seq[1:n]/a_pen0), crossval_nsim))
## End of simulation of crossval_nsim paths
p.vec = seq(0,0.01,0.001) ## Try this range of values for pen. For the AMOC time series in DD23, 0.004 was the optimal value.
MSE = matrix(0,ncol = 2, nrow = length(p.vec))
dimnames(MSE)[[2]] = c("p","MSE")
MSE[,1] = p.vec
for(i in 1:length(p.vec)){ ## For every choice of pen, determine the distribution of estimated tau_r
pen = p.vec[i]
estim.tau = numeric(crossval_nsim) ## Vector to hold estimates
for(isim in 1:crossval_nsim){
xxi   = crossval_xx.data[crossval_xx.data$time > t0 & crossval_xx.data$repetition == isim,"X"]  ##Get data from isim simulation
xxi.0 = crossval_xx.data[crossval_xx.data$time <= t0 & crossval_xx.data$repetition == isim,"X"] ##Get data from isim simulation
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.tau[isim] = temp2$par["tau"]
}
MSE[i,2] = mean((estim.tau-tau_pen0)^2) ## The distributions are assessed in their square error w.r.t. the fitting target at pen = 0
}
## Do full tipping time analysis on all time series with optimal value of penalization p
pen = p.vec[which.min(c(MSE[,2]))] ## pen is chosen as the value where the above square error was minimal
print(paste("Optimal pen:",pen))
}## End of penalization routine
estim.matrix = matrix(0, ncol = 8, nrow = nsim) ## Matrix to hold estimates
dimnames(estim.matrix)[[2]] = c("alpha0","mu0","lambda0", "tau", "s2", "m", "a", "tc")
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
estim.matrix[isim,"alpha0"] = coef(temp1)[1]
estim.matrix[isim,"mu0"]    = coef(temp1)[2]
estim.matrix[isim,"s2"]     = coef(temp1)[3]
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.matrix[isim,"tau"] = temp2$par[1]
estim.matrix[isim, "a"]  = temp2$par[2]
}
estim.matrix[,"m"] = estim.matrix[,"mu0"] - estim.matrix[,"alpha0"]/(2*estim.matrix[,"a"])
estim.matrix[,"lambda0"] = - estim.matrix[,"alpha0"]^2/(4*estim.matrix[,"a"])
estim.matrix[,"tc"] = estim.matrix[,"tau"] + t0
write_xlsx(as.data.frame(estim.matrix), paste("EstimMatrix_",modelname,".xlsx",sep=""))
}
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
take_sample = 2
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
## read the desired sample path
xx = data.matrix(read.csv(filepath))[,-1]
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## Generate the qq-plot, directly adopted from DD23.
n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
time       = Delta_t*(1:(n-1))
lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*Delta_t)
mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
norm.resid.S = m.part/sd.seq
write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
qqnorm(norm.resid.S, main = modelname)
qqline(norm.resid.S)
}
load("SimulatedTraces.Rdata")
p.vec = c(0,0.004,0.01) ## Try this range of values for pen. For the AMOC time series in DD23, 0.004 was the optimal value.
all.estim.matrix = matrix(0, ncol = 9, nrow = 0)
dimnames(all.estim.matrix)[[2]] = c("alpha0","mu0","lambda0", "tau", "s2", "m", "a", "tc","pen")
for(i in 1:length(p.vec)){
pen = p.vec[i]
estim.matrix = matrix(0, ncol = 9, nrow = nsim) ## Matrix to hold estimates
dimnames(estim.matrix)[[2]] = c("alpha0","mu0","lambda0", "tau", "s2", "m", "a", "tc","pen")
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
estim.matrix[isim,"alpha0"] = coef(temp1)[1]
estim.matrix[isim,"mu0"]    = coef(temp1)[2]
estim.matrix[isim,"s2"]     = coef(temp1)[3]
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.matrix[isim,"tau"] = temp2$par[1]
estim.matrix[isim, "a"]  = temp2$par[2]
}
estim.matrix[,"m"] = estim.matrix[,"mu0"] - estim.matrix[,"alpha0"]/(2*estim.matrix[,"a"])
estim.matrix[,"lambda0"] = - estim.matrix[,"alpha0"]^2/(4*estim.matrix[,"a"])
estim.matrix[,"tc"] = estim.matrix[,"tau"] + t0
estim.matrix[,"pen"] = rep(pen,nsim)
all.estim.matrix = rbind(all.estim.matrix,estim.matrix)
}
write_xlsx(as.data.frame(all.estim.matrix), "EstimMatrix_penalization.xlsx")
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
take_sample = 1
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
## read the desired sample path
xx = data.matrix(read.csv(filepath))[,-1]
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## Generate the qq-plot, directly adopted from DD23.
n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
time       = Delta_t*(1:(n-1))
lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*Delta_t)
mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
norm.resid.S = m.part/sd.seq
write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
qqnorm(norm.resid.S, main = modelname)
qqline(norm.resid.S)
}
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
take_sample = 1000
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
## read the desired sample path
xx = data.matrix(read.csv(filepath))[,-1]
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## Generate the qq-plot, directly adopted from DD23.
n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
time       = Delta_t*(1:(n-1))
lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*Delta_t)
mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
norm.resid.S = m.part/sd.seq
write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
qqnorm(norm.resid.S, main = modelname)
qqline(norm.resid.S)
}
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
take_sample = 3
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
## read the desired sample path
xx = data.matrix(read.csv(filepath))[,-1]
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## Generate the qq-plot, directly adopted from DD23.
n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
time       = Delta_t*(1:(n-1))
lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*Delta_t)
mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
norm.resid.S = m.part/sd.seq
write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
qqnorm(norm.resid.S, main = modelname)
qqline(norm.resid.S)
}
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
take_sample = 4
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
## read the desired sample path
xx = data.matrix(read.csv(filepath))[,-1]
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## Generate the qq-plot, directly adopted from DD23.
n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
time       = Delta_t*(1:(n-1))
lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*Delta_t)
mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
norm.resid.S = m.part/sd.seq
write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
qqnorm(norm.resid.S, main = modelname)
qqline(norm.resid.S)
}
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
take_sample = 5
n = round((2021-(1870+1/12))*12+1)
Delta_t = 1/12
estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
## read the desired sample path
xx = data.matrix(read.csv(filepath))[,-1]
xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## Generate the qq-plot, directly adopted from DD23.
n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
time       = Delta_t*(1:(n-1))
lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*Delta_t)
mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
norm.resid.S = m.part/sd.seq
write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
qqnorm(norm.resid.S, main = modelname)
qqline(norm.resid.S)
}
