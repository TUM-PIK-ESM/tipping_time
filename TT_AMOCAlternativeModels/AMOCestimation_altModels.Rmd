---
title: "Applying the MLE method of Ditlevsen and Ditlevsen (2023) to estimate tipping times in data from different AMOC models"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    code_folding: show
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE, 
                      class.source = 'fold-hide')

```

This code is largely based on the Suppplementary Material of Ditlevsen and Ditlevsen (2023) [DD23]. Running this script on data from 10000 sample paths as in Ben-Yami 2024 takes about 8h on an Apple M1 chip. The code is written in R:

R Core Team (2022). R: A language and environment for statistical computing. R Foundation for
  Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.


## Preliminaries

The following code chunk loads the required R packages.

```{r }

library(ggplot2)   ## For nice plotting
library(stats4)    ## For maximum likelihood facilities (mle())
library(writexl)   ## For writing data frames into excel files
library(readxl)    ## For reading data frames into excel files
library(stringr)   ## For manipulating strings
```

The following chunk code defines functions for likelihood calculations and simulations. Taken from DD23 without changes.

```{r }
## Function returning 2 x the negative log-likelihood of a trace up to a constant
## The trace is assumed an Ornstein-Uhlenbeck with constant mean mu and rate
## parameter alpha. 
loglikOU.fun = function(pars, data, delta){
  ## pars = (alpha, mu, sigma) are the parameters to be estimated in the model:
  ## dXt = - alpha * ( Xt - mu ) dt + sigma * dWt
  ## data is a trace 
  ## delta is the time step between observations
  alpha0 = max(pars[1],0.001) #Ensuring alpha is positive
  mu0    = pars[2]            #Mean
  sigma2 = max(0,pars[3])     #Infinitisimal variance, should be positive
  n      = length(data)
  Xupp   = data[2:n]
  Xlow   = data[1:(n-1)]
  time   = delta*(1:(n-1))
  gamma2 = sigma2/(2*alpha0)  #Asymptotic variance
  rho0   = exp(-alpha0*delta) #Autocorrelation
  m.part = Xupp - Xlow*rho0 - mu0*(1-rho0) #Observation minus the mean of transition distribution
  v.part = gamma2*(1-rho0^2) #Variance of transition distribution
  loglik = - n*(log(v.part)) - sum(m.part^2/v.part)
  -loglik
}

## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations
estimate.OU = function(data, delta, initial.values = NULL){
  if(is.null(initial.values)){
    n    = length(data)
    Xupp = data[2:n]
    Xlow = data[1:(n-1)]
    mu   = mean(data) ##Starting value for mu0 
    alpha = -log(sum((Xupp-mu)*(Xlow-mu))/sum((Xlow-mu)^2))/delta ##Starting value for alpha0 
    s2 = mean(diff(data)^2)/delta ##Quadratic variation, starting value for sigma^2
    par.init = c(alpha, mu, s2)   ## Collect starting values
  }
  if(!is.null(initial.values)) par.init = initial.values
  minuslogl = function(alpha,mu,sigma2){
    logLik = tryCatch(loglikOU.fun(pars = c(alpha,mu,sigma2), data = data, delta = delta))
    if(is.na(logLik))
      {
        return(10000)
      }else
      {
        return(logLik)
      }
    }
  temp = stats4::mle(minuslogl = minuslogl, 
             start = list(alpha = par.init[1], mu = par.init[2], sigma2 = par.init[3]))
  return(temp)
}

## Function returning 2 x the negative log-likelihood of a trace up to a constant from model
## dXt = - (a*(Xt - m)^2 + lambda) dt + sigma * dWt
## Assuming alpha0, mu0, sigma known
## Strang splitting estimator based on Taylor expansion around mu(lambda)
loglik.fun = function(pars, data, delta, alpha0, mu0, sigma20, pen = 0){
  ##pars are the parameters to be estimated, tau and a
  ##data is a trace under linear changing of lambda
  ##delta is the time step between observations
  ##alpha0, mu0, sigma20 are parameters already estimated in the OU process from stationary part
  tau     = pars[1]             #Ramping time
  a       = max(0.1,pars[2])    #Factor in front of (x-m)^2 in drift. Positive
  m       = mu0 - alpha0/(2*a)  #Constant mean shift
  lambda0 = -alpha0^2/(4*a)     #Stationary level of control parameter
  sigma2  = sigma20             #Infinitisimal variance
  n       = length(data)
  Xupp    = data[2:n]
  Xlow    = data[1:(n-1)]
  time    = delta*(1:(n-1))
  lam.seq    = lambda0*(1-time/tau)
  alpha.seq  = 2*sqrt(-a*lam.seq)
  gamma2.seq = sigma2/(2*alpha.seq)
  rho.seq    = exp(-alpha.seq*delta)
  mu.seq     = m + sqrt(-lam.seq/a)
  ## Calculating the Strang splitting scheme pseudo likelihood
    fh.half.tmp = a*delta*(Xlow - mu.seq)/2
    fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
    fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
    mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
    m.part      = fh.half.inv - mu.h
    var.part    = gamma2.seq*(1-rho.seq^2)
    det.Dfh.half.inv = 1/(a*delta*(Xupp-mu.seq)/2-1)^2
    loglik      = - sum(log(var.part)) - sum(m.part^2/var.part) + 
      2*sum(log(det.Dfh.half.inv)) - pen*n*(1/a - 1)*(a < 1)
  return(-loglik)
}

## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations, and time passed
## at present since time t_0
estimate.tipping = function(data, delta, initial.values = c(100,1),  
                            alpha0, mu0, sigma20, pen = pen){
  par.init = initial.values
  minuslogl = function(pars){
    logLik = tryCatch(loglik.fun(pars = pars, data = data, delta = delta,
               alpha0 = alpha0, mu0 = mu0, sigma20 = sigma20, pen = pen))
    if(is.na(logLik))
      {
        return(10000)
      }else
      {
        return(logLik)
      }
    }
  temp = optim(par = c(tau = par.init[1], a = par.init[2]), 
               fn = minuslogl, method = "Nelder-Mead", hessian = FALSE)
  return(temp)
}

## Function returning a trajectory up to a crossing time of X(t)  
## over the barrier as a function of
## the initial condition X0, the size of the noise sigma, 
## the integration time step dt, and the length of the simulation N
## and with time varying lambda:
X.traj <- function(sigma = 0.1, lambda0 = -2, tau = 1000, m = 0, a = 1,  
                   T0 = 0, X0 = sqrt(2), dt = 0.1, Ymax = 1000000){
  ##T0: Time before ramping starts
  ##Ymax: Max number of simulated points, if tipping does not happen
  Y = 0  ## Counting integration steps up to tipping
  xbarrier = m - 2 ## One smaller than crossing point at start
  Xtraj = X0
  X = X0
  ## Simulation during stationary period, constant lambda
  while(X > xbarrier & Y < T0/dt){
    X = S.onestep(sigma = sigma, lambda = lambda0,  
                  m = m, a = a, X0 = X, dt = dt)
    Xtraj = c(Xtraj, X)
    Y = Y+1
  }
  ## Simulation after lambda starts increasing
  while(X > xbarrier & Y < Ymax){
    time = dt * (Y - T0/dt) ## Time after lambda starts increasing
    lambda = lambda0*(1-time/tau)
    X = S.onestep(sigma = sigma, lambda = lambda0*(1 - time/tau), 
                  m = m, a = a, X0 = X, dt = dt)
    Xtraj = c(Xtraj, X)
    Y = Y + 1
  }
  Y = Y*dt
  return(list(FPT = Y, X = Xtraj))
}

## Function returning a simulation of the simple model one time step
## using the Euler-Maruyama scheme
S.onestep <- function(sigma = 0.1, lambda = 0, m = 0, a = 1, X0 = 1, dt = 0.1){
  dWt = rnorm(1,0,1)    ## Draws a random increment
  Y   = X0 - a*(X0-m)^2*dt - lambda*dt + sqrt(dt)*sigma*dWt
  return(Y)
}

```

## Estimating tipping time on synthetic model data

Time series of different synthetic models are read from txt files generated in the accompanying ipynb file. On each time series, the parameter estimation routine introduced by DD23 is applied and the results are written into xlsx files. Plots are generated in the aforementioned ipynb file.

```{r }

## Take the estimates from the AMOC data
t0 = 1924 ## Knowledge of t0 is presupposed. All synthetic models have the same true value of t0 = 1924.
#alpha0 = 3.06
#mu0 = 0.25
#sigma2 = 0.3/9
#tau = 132.52
#a = 0.87
#m = -1.51
#lambda0 = -2.69
#tc = 2056.52

##############################
## Estimation on each trace ##
##############################


for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
  modelname = str_sub(filepath,16,-5) 
  print(modelname)
  n = round((2021-(1870+1/12))*12+1)
  Delta_t = 1/12

  xx = data.matrix(read.csv(filepath))[,-1]
  nsim = dim(xx)[2]   ## Number of simulations
  xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
  xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
                       repetition = rep(1:nsim, each = n))
  ## xx.data now contains nsim time series of the current examined model
  
  
  pen = 0 ### Penalization is set to 0 by default. Larger penalization would lead to even lower estimated tipping times. 
  
  
  estim.matrix = matrix(0, ncol = 8, nrow = nsim) ## Matrix to hold estimates
  dimnames(estim.matrix)[[2]] = c("alpha0","mu0","lambda0", "tau", "s2", "m", "a", "tc")
  
  for(isim in 1:nsim){
    ## Get data from isim simulation after time t0, when linear ramping has started
    xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]  
    xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
    ## Get baseline data from isim simulation, before time t0
    xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"] 
    nx = length(xxi)
    
    temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
    estim.matrix[isim,"alpha0"] = coef(temp1)[1]
    estim.matrix[isim,"mu0"]    = coef(temp1)[2]
    estim.matrix[isim,"s2"]     = coef(temp1)[3]
  
    temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(100, 1), 
              alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
    estim.matrix[isim,"tau"] = temp2$par[1]
    estim.matrix[isim, "a"]  = temp2$par[2]
  }
  
  estim.matrix[,"m"] = estim.matrix[,"mu0"] - estim.matrix[,"alpha0"]/(2*estim.matrix[,"a"])
  estim.matrix[,"lambda0"] = - estim.matrix[,"alpha0"]^2/(4*estim.matrix[,"a"])
  estim.matrix[,"tc"] = estim.matrix[,"tau"] + t0
  
  
  write_xlsx(as.data.frame(estim.matrix), paste("EstimMatrix_",modelname,".xlsx",sep=""))
}

```

## Checking the model with normal residuals

Here, residuals are calculated and plotted in a qq-plot, similar to Figure 6 in Ditlevsen and Ditlevsen (2023). The qq-plot shows a good fit to the fold normal form white noise model of DD23.

```{r, eval = TRUE }
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file

  modelname = str_sub(filepath,16,-5)
  if (modelname=="sc_wn_linforc_1")
  {
    take_sample = 22
  } else if(modelname=="sc_wn_linforc_mn_1"||  modelname=="sc_wn_linforc_mn_2" || modelname=="sc_wn_linforc_mn_3")
  {
    take_sample = 22
  } else 
  {
    take_sample = 34
  }
  n = round((2021-(1870+1/12))*12+1)
  Delta_t = 1/12
  
  estim.matrix = data.matrix(read_xlsx(paste("EstimMatrix_",modelname,".xlsx", sep = ""))) ## get the estimation values of the above analysis
  print(estim.matrix[take_sample,"tc"])
  ## read the desired sample path
  xx = data.matrix(read.csv(filepath))[,-1]
  xxtime = seq(from = 1870+1/12, to = 2021, length.out=n)
  xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
                       repetition = rep(1:nsim, each = n))
  
  ## Generate the qq-plot, directly adopted from DD23.
  n          = length(xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"])
  Xupp       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][2:n]
  Xlow       = xx.data[xx.data$time > t0 & xx.data$repetition == take_sample,"X"][1:(n-1)]
  time       = Delta_t*(1:(n-1))
  lam.seq    = estim.matrix[take_sample,"lambda0"]*(1-pmax(0,(time-t0))/estim.matrix[take_sample,"tau"])
  alpha.seq  = 2*sqrt(-estim.matrix[take_sample,"a"]*lam.seq)
  mu.seq     = estim.matrix[take_sample,"m"] + sqrt(-lam.seq/estim.matrix[take_sample,"a"])
  gamma2.seq = estim.matrix[take_sample,"s2"]/(2*alpha.seq)
  rho.seq    = exp(-alpha.seq*Delta_t)
  
  mean1.tmp  = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)
  mean2.tmp  = (mu.seq*mean1.tmp+Xlow)/(mean1.tmp+1)
  mean.cond  = mean2.tmp*rho.seq + mu.seq*(1-rho.seq)
  sd.seq     = sqrt(gamma2.seq*(1-rho.seq^2))
  
  fh.half.tmp = estim.matrix[take_sample,"a"]*Delta_t*(Xlow - mu.seq)/2
  fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
  fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
  mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
  m.part      = fh.half.inv - mu.h
  var.part    = gamma2.seq*(1-rho.seq^2)
  norm.resid.S = m.part/sd.seq
  write.csv(norm.resid.S,paste("QQData_",modelname,".csv",sep="")) ## Write the data into a csv file to plot in the ipynb file
  qqnorm(norm.resid.S, main = modelname)
  qqline(norm.resid.S)
}

```