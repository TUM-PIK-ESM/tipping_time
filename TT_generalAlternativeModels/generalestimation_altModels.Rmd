---
title: "Applying the MLE method of Ditlevsen and Ditlevsen (2023) and linear extrapolation of EWS to estimate tipping times in data from different conceptual models"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    code_folding: show
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE, 
                      class.source = 'fold-hide')

```


This code is largely based on the Suppplementary Material of Ditlevsen and Ditlevsen (2023) [DD23]. Running this script on data from 10000 sample paths as in Ben-Yami 2024 takes about 5h on an Apple M1 chip. The code is written in R:

R Core Team (2022). R: A language and environment for statistical computing. R Foundation for
  Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.


## Preliminaries

The following code chunk loads the required R packages.

```{r }

library(ggplot2)   ## For nice plotting
library(stats4)    ## For maximum likelihood facilities (mle())
library(writexl)   ## For writing data frames into excel files
library(readxl)    ## For reading data frames into excel files
library(stringr)   ## For manipulating strings
library(zoo)
library(astsa)
library(reticulate)
use_python("/opt/homebrew/Caskroom/miniforge/base/bin/python")
sm <- import("statsmodels.api")
source_python("EstimationMethods.py")
```

The following chunk code defines functions for likelihood calculations and simulations. Taken from DD23 without changes.

```{r }
## Function returning 2 x the negative log-likelihood of a trace up to a constant
## The trace is assumed an Ornstein-Uhlenbeck with constant mean mu and rate
## parameter alpha. 
loglikOU.fun = function(pars, data, delta){
  ## pars = (alpha, mu, sigma) are the parameters to be estimated in the model:
  ## dXt = - alpha * ( Xt - mu ) dt + sigma * dWt
  ## data is a trace 
  ## delta is the time step between observations
  alpha0 = max(pars[1],0.001) #Ensuring alpha is positive
  mu0    = pars[2]            #Mean
  sigma2 = max(0,pars[3])     #Infinitisimal variance, should be positive
  n      = length(data)
  Xupp   = data[2:n]
  Xlow   = data[1:(n-1)]
  time   = delta*(1:(n-1))
  gamma2 = sigma2/(2*alpha0)  #Asymptotic variance
  rho0   = exp(-alpha0*delta) #Autocorrelation
  m.part = Xupp - Xlow*rho0 - mu0*(1-rho0) #Observation minus the mean of transition distribution
  v.part = gamma2*(1-rho0^2) #Variance of transition distribution
  loglik = - n*(log(v.part)) - sum(m.part^2/v.part)
  -loglik
}

## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations
estimate.OU = function(data, delta, initial.values = NULL){
  if(is.null(initial.values)){
    n    = length(data)
    Xupp = data[2:n]
    Xlow = data[1:(n-1)]
    mu   = mean(data) ##Starting value for mu0 
    alpha = -log(sum((Xupp-mu)*(Xlow-mu))/sum((Xlow-mu)^2))/delta ##Starting value for alpha0 
    s2 = mean(diff(data)^2)/delta ##Quadratic variation, starting value for sigma^2
    par.init = c(alpha, mu, s2)   ## Collect starting values
  }
  if(!is.null(initial.values)) par.init = initial.values
  minuslogl = function(alpha,mu,sigma2){
    logLik = tryCatch(loglikOU.fun(pars = c(alpha,mu,sigma2), data = data, delta = delta))
    if(is.na(logLik))
      {
        return(10000)
      }else
      {
        return(logLik)
      }
    }
  temp = stats4::mle(minuslogl = minuslogl, 
             start = list(alpha = par.init[1], mu = par.init[2], sigma2 = par.init[3]))
  return(temp)
}

## Function returning 2 x the negative log-likelihood of a trace up to a constant from model
## dXt = - (a*(Xt - m)^2 + lambda) dt + sigma * dWt
## Assuming alpha0, mu0, sigma known
## Strang splitting estimator based on Taylor expansion around mu(lambda)
loglik.fun = function(pars, data, delta, alpha0, mu0, sigma20, pen = 0){
  ##pars are the parameters to be estimated, tau and a
  ##data is a trace under linear changing of lambda
  ##delta is the time step between observations
  ##alpha0, mu0, sigma20 are parameters already estimated in the OU process from stationary part
  tau     = pars[1]             #Ramping time
  a       = max(0.1,pars[2])    #Factor in front of (x-m)^2 in drift. Positive
  m       = mu0 - alpha0/(2*a)  #Constant mean shift
  lambda0 = -alpha0^2/(4*a)     #Stationary level of control parameter
  sigma2  = sigma20             #Infinitisimal variance
  n       = length(data)
  Xupp    = data[2:n]
  Xlow    = data[1:(n-1)]
  time    = delta*(1:(n-1))
  lam.seq    = lambda0*(1-time/tau)
  alpha.seq  = 2*sqrt(-a*lam.seq)
  gamma2.seq = sigma2/(2*alpha.seq)
  rho.seq    = exp(-alpha.seq*delta)
  mu.seq     = m + sqrt(-lam.seq/a)
  ## Calculating the Strang splitting scheme pseudo likelihood
    fh.half.tmp = a*delta*(Xlow - mu.seq)/2
    fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
    fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
    mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
    m.part      = fh.half.inv - mu.h
    var.part    = gamma2.seq*(1-rho.seq^2)
    det.Dfh.half.inv = 1/(a*delta*(Xupp-mu.seq)/2-1)^2
    loglik      = - sum(log(var.part)) - sum(m.part^2/var.part) + 
      2*sum(log(det.Dfh.half.inv)) - pen*n*(1/a - 1)*(a < 1)
  return(-loglik)
}

## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations, and time passed
## at present since time t_0
estimate.tipping = function(data, delta, initial.values = c(100,1),  
                            alpha0, mu0, sigma20, pen = pen){
  par.init = initial.values
  minuslogl = function(pars){
    logLik = tryCatch(loglik.fun(pars = pars, data = data, delta = delta,
               alpha0 = alpha0, mu0 = mu0, sigma20 = sigma20, pen = pen))
    if(is.na(logLik))
      {
        return(10000)
      }else
      {
        return(logLik)
      }
    }
  temp = optim(par = c(tau = par.init[1], a = par.init[2]), 
               fn = minuslogl, method = "Nelder-Mead", hessian = FALSE)
  return(temp)
}

## Function returning a trajectory up to a crossing time of X(t)  
## over the barrier as a function of
## the initial condition X0, the size of the noise sigma, 
## the integration time step dt, and the length of the simulation N
## and with time varying lambda:
X.traj <- function(sigma = 0.1, lambda0 = -2, tau = 1000, m = 0, a = 1,  
                   T0 = 0, X0 = sqrt(2), dt = 0.1, Ymax = 1000000){
  ##T0: Time before ramping starts
  ##Ymax: Max number of simulated points, if tipping does not happen
  Y = 0  ## Counting integration steps up to tipping
  xbarrier = m - 2 ## One smaller than crossing point at start
  Xtraj = X0
  X = X0
  ## Simulation during stationary period, constant lambda
  while(X > xbarrier & Y < T0/dt){
    X = S.onestep(sigma = sigma, lambda = lambda0,  
                  m = m, a = a, X0 = X, dt = dt)
    Xtraj = c(Xtraj, X)
    Y = Y+1
  }
  ## Simulation after lambda starts increasing
  while(X > xbarrier & Y < Ymax){
    time = dt * (Y - T0/dt) ## Time after lambda starts increasing
    lambda = lambda0*(1-time/tau)
    X = S.onestep(sigma = sigma, lambda = lambda0*(1 - time/tau), 
                  m = m, a = a, X0 = X, dt = dt)
    Xtraj = c(Xtraj, X)
    Y = Y + 1
  }
  Y = Y*dt
  return(list(FPT = Y, X = Xtraj))
}

## Function returning a simulation of the simple model one time step
## using the Euler-Maruyama scheme
S.onestep <- function(sigma = 0.1, lambda = 0, m = 0, a = 1, X0 = 1, dt = 0.1){
  dWt = rnorm(1,0,1)    ## Draws a random increment
  Y   = X0 - a*(X0-m)^2*dt - lambda*dt + sqrt(dt)*sigma*dWt
  return(Y)
}

```

Defining alternative methods of estimating tipping time by linearly extrapolating trends in CSD indicators
```{r }

### Functions for indicator time series later used for extrapolation towards a tipping point. The estimator of the generalised least square estimator used in Boers2021 is imported from the python module EstimationMethods.py
var_fun = function(data){
  data = detrend(data, order = 2)
  this.var = var(data)
  return(this.var)
}

ac1_fun = function(data){
  data = detrend(data, order = 2)
  this.var = var(data)
  this.acov = mean(data[2:length(data)]*data[1:(length(data)-1)])
  this.ac1 = this.acov/this.var
  return(this.ac1)
}

### This function performs a linear extrapolation of a specified indicator time series on data to retrieve a tipping time estimate
estimate.tipping.linearTrend = function(data, csdindicator, windowsize, jump){
  if(csdindicator=="variance_inv")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = var_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = 1/indicator.series
    } else if(csdindicator=="ac1")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = ac1_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = 1-indicator.series
    } else if(csdindicator=="ac1_log")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = ac1_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = log(indicator.series)
    } else if(csdindicator=="gls")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = phi_gls, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = 1-indicator.series
    } else if(csdindicator=="gls_log")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = phi_gls, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = log(indicator.series)
    } else if(csdindicator=="variance_inv_sq")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = var_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = (1/indicator.series)**2
    } else if(csdindicator=="ac1_log_sq")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = ac1_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = log(indicator.series)**2
    } else if(csdindicator=="gls_log_sq")
    {
      indicator.series = rollapply(data, width = windowsize, FUN = phi_gls, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
      control.parameter.series = log(indicator.series)**2
    }
  linear.fit.time = 1:length(data)
  linear.fit = lm(formula = control.parameter.series ~ linear.fit.time)
  tau.linear = -coefficients(linear.fit)[1]/coefficients(linear.fit)[2]
  if (tau.linear<0)
  {
    tau.linear = 10**6 ### Default value for "infinite" estimated tipping time
  }
  return(tau.linear)
}
```
## Estimating tipping time on synthetic model data

Time series of different synthetic models are read from txt files generated in the accompanying ipynb file. On each time series, the parameter estimation routine introduced by DD23 as well as the the linear extrapolation of EWS is applied and the results are written into xlsx files. Plots are generated in the aforementioned ipynb file.

```{r }
t0 = 1000 ## Knowledge of t0 is presupposed.

##############################
## Estimation on each trace ##
##############################


for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
  modelname = str_sub(filepath,16,-5) 
  print(modelname)
  n = 2000
  Delta_t = 1

  xx = data.matrix(read.csv(filepath))[,-1]
  nsim = dim(xx)[2]   ## Number of simulations
  xxtime = seq(from = 1, to = n, length.out=n)
  xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
                       repetition = rep(1:nsim, each = n))
  ## xx.data now contains nsim time series of the current examined model
  pen = 0 ### Penalization is set to 0 by default. Larger penalization would lead to even lower estimated tipping times. 


  estim.matrix = matrix(0, ncol = 9, nrow = nsim) ## Matrix to hold estimates
  dimnames(estim.matrix)[[2]] = c("variance_inv","variance_inv_sq","ac1","ac1_log","ac1_log_sq","gls","gls_log","gls_log_sq","dd23")
  
  for(isim in 1:nsim){
    ## Get data from isim simulation after time t0, when linear ramping has started
    xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]  
    xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
    ## Get baseline data from isim simulation, before time t0
    xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"] 
    nx = length(xxi)
    
    temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
  
    temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1), 
              alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
    estim.matrix[isim,"dd23"] = temp2$par[1] + t0
    
    windowsize = 100
    jump = 50
    
    ### Specify here which indicator time series should be linearly extrapolated to retrieve a tipping time estimate
    #estim.matrix[isim,"variance_inv"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "variance_inv", windowsize = windowsize, jump = jump) + t0
    #estim.matrix[isim,"variance_inv_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "variance_inv_sq", windowsize = windowsize, jump = jump) + t0
    #estim.matrix[isim,"ac1"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1", windowsize = windowsize, jump = jump) + t0
    #estim.matrix[isim,"ac1_log"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1_log", windowsize = windowsize, jump = jump) + t0
    estim.matrix[isim,"ac1_log_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1_log_sq", windowsize = windowsize, jump = jump) + t0
    #estim.matrix[isim,"gls"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls", windowsize = windowsize, jump = jump) + t0
    #estim.matrix[isim,"gls_log"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls_log", windowsize = windowsize, jump = jump) + t0
    estim.matrix[isim,"gls_log_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls_log_sq", windowsize = windowsize, jump = jump) + t0
    
  }

  
  
  write_xlsx(as.data.frame(estim.matrix), paste("TipEstimResults_",modelname,".xlsx",sep=""))
}

```
