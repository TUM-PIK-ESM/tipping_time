##############################
## Estimation on each trace ##
##############################
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
print(modelname)
n = 2000
Delta_t = 1
xx = data.matrix(read.csv(filepath))[,-1]
nsim = dim(xx)[2]   ## Number of simulations
xxtime = seq(from = 1, to = n, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## xx.data now contains nsim time series of the current examined model
pen = 0
run_penalization_routine = FALSE ## If TRUE the optimal pen as the value with least spread around the tc estimation corresponding to pen = 0
if (run_penalization_routine){
## First with no penalization, determine the estimated tau_r
estim.alpha0 = numeric(nsim)
estim.mu0 = numeric(nsim)
estim.s2 = numeric(nsim)
estim.tau = numeric(nsim)
estim.a = numeric(nsim)
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
estim.alpha0[isim] = coef(temp1)[1]
estim.mu0[isim]   = coef(temp1)[2]
estim.s2[isim]     = coef(temp1)[3]
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.tau[isim] = temp2$par[1]
estim.a[isim]  = temp2$par[2]
}
alpha0_pen0 = median(estim.alpha0)
mu0_pen0 = median(estim.mu0)
s2_pen0 = median(estim.s2)
tau_pen0 = median(estim.tau) ## This value is the fitting target for the subsequent search of an optimal pen
a_pen0 = median(estim.a)
m_pen0 = median(estim.mu0 - estim.alpha0/(2*estim.a))
lambda0_pen0 = median(- estim.alpha0^2/(4*estim.a))
tc_pen0 = median(estim.tau + t0)
#print(paste("alpha0_pen0:",alpha0_pen0))
#print(paste("mu0_pen0:",mu0_pen0))
#print(paste("s2_pen0:",s2_pen0))
#print(paste("tau_pen0:",tau_pen0))
#print(paste("a_pen0:",a_pen0))
#print(paste("m_pen0:",m_pen0))
#print(paste("lambda0_pen0:",lambda0_pen0))
#print(paste("tc_pen0:",tc_pen0))
#Define parameters
crossval_nsim = 50 ## Number of cross validation time series
T0      = t0      ## Time length of stationary dynamics (lambda constant)
sig     = sqrt(s2_pen0) ## Infinitisimal standard deviation: sigma
time    = seq(from = 1, to = n, length.out=n)
lam.seq = lambda0_pen0*(1-pmax(0,(time-t0))/tau_pen0) ## Time varying lambda
alpha.seq = 2*sqrt(-a_pen0*lam.seq) ## Time varying alpha
mu.seq  = m_pen0 + sqrt(-lam.seq/a_pen0) ## Time varying mu
var0    = s2_pen0/(2*alpha0_pen0)    ## Time varying variance
Delta_t = 1                 ## Time step in years (one measurement per month)
nloop   = 20                   ## Number of simulation steps between observation steps
tp      = 1000 - t0            ## Time length of non-stationary dynamics (lambda linearly increasing)
nt      = ceiling(tp/Delta_t) ## Total number of points from t_0 to today
## Generate synthetic time series to find the optimal pen on
crossval_xx = matrix(NA, n, crossval_nsim)
for (isim in 1:crossval_nsim){
#Initial condition is random from stationary distribution
x0 = m_pen0 + sqrt(-lambda0_pen0/a_pen0) + rnorm(1, mean = 0, sd = sig/sqrt(2*alpha0_pen0))
xxx = X.traj(sigma = sig, lambda0 = lambda0_pen0, tau = tau_pen0, m = m_pen0, a = a_pen0,
T0 = T0, X0 = x0, dt = Delta_t/nloop,
Ymax = nloop*n-1)
nx = length(xxx$X) ##If tipping happens before, trajectory is shorter than nt
xxxx = xxx$X[seq(1, nx, nloop)] ##Only keep points at observed times
nxx = length(xxxx)
crossval_xx[1:nxx, isim] = xxxx
}
xxtime <- time[1:n]
crossval_xx.data = data.frame(X = c(crossval_xx), time = rep(xxtime, crossval_nsim),
repetition = rep(1:crossval_nsim, each = n),
model = rep(m_pen0 + sqrt(-lam.seq[1:n]/a_pen0), crossval_nsim))
## End of simulation of crossval_nsim paths
p.vec = seq(0,0.01,0.001) ## Try this range of values for pen. For the AMOC time series in DD23, 0.004 was the optimal value.
MSE = matrix(0,ncol = 2, nrow = length(p.vec))
dimnames(MSE)[[2]] = c("p","MSE")
MSE[,1] = p.vec
for(i in 1:length(p.vec)){ ## For every choice of pen, determine the distribution of estimated tau_r
pen = p.vec[i]
estim.tau = numeric(crossval_nsim) ## Vector to hold estimates
for(isim in 1:crossval_nsim){
xxi   = crossval_xx.data[crossval_xx.data$time > t0 & crossval_xx.data$repetition == isim,"X"]  ##Get data from isim simulation
xxi.0 = crossval_xx.data[crossval_xx.data$time <= t0 & crossval_xx.data$repetition == isim,"X"] ##Get data from isim simulation
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.tau[isim] = temp2$par["tau"]
}
MSE[i,2] = mean((estim.tau-tau_pen0)^2) ## The distributions are assessed in their square error w.r.t. the fitting target at pen = 0
}
## Do full tipping time analysis on all time series with optimal value of penalization p
pen = p.vec[which.min(c(MSE[,2]))] ## pen is chosen as the value where the above square error was minimal
print(paste("Optimal pen:",pen))
}## End of penalization routine
estim.matrix = matrix(0, ncol = 9, nrow = nsim) ## Matrix to hold estimates
dimnames(estim.matrix)[[2]] = c("variance_inv","variance_inv_sq","ac1","ac1_log","ac1_log_sq","gls","gls_log","gls_log_sq","dd23")
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.matrix[isim,"dd23"] = temp2$par[1] + t0
windowsize = 100
jump = 10#50
#estim.matrix[isim,"variance_inv"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "variance_inv", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"variance_inv_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "variance_inv_sq", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"ac1"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"ac1_log"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1_log", windowsize = windowsize, jump = jump) + t0
estim.matrix[isim,"ac1_log_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1_log_sq", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"gls"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"gls_log"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls_log", windowsize = windowsize, jump = jump) + t0
estim.matrix[isim,"gls_log_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls_log_sq", windowsize = windowsize, jump = jump) + t0
}
write_xlsx(as.data.frame(estim.matrix), paste("TipEstimResults_",modelname,".xlsx",sep=""))
}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE,
class.source = 'fold-hide')
library(ggplot2)   ## For nice plotting
library(stats4)    ## For maximum likelihood facilities (mle())
library(writexl)   ## For writing data frames into excel files
library(readxl)    ## For reading data frames into excel files
library(stringr)   ## For manipulating strings
library(zoo)
library(astsa)
library(reticulate)
use_python("/opt/homebrew/Caskroom/miniforge/base/bin/python")
sm <- import("statsmodels.api")
source_python("EstimationMethods.py")
## Function returning 2 x the negative log-likelihood of a trace up to a constant
## The trace is assumed an Ornstein-Uhlenbeck with constant mean mu and rate
## parameter alpha.
loglikOU.fun = function(pars, data, delta){
## pars = (alpha, mu, sigma) are the parameters to be estimated in the model:
## dXt = - alpha * ( Xt - mu ) dt + sigma * dWt
## data is a trace
## delta is the time step between observations
alpha0 = max(pars[1],0.001) #Ensuring alpha is positive
mu0    = pars[2]            #Mean
sigma2 = max(0,pars[3])     #Infinitisimal variance, should be positive
n      = length(data)
Xupp   = data[2:n]
Xlow   = data[1:(n-1)]
time   = delta*(1:(n-1))
gamma2 = sigma2/(2*alpha0)  #Asymptotic variance
rho0   = exp(-alpha0*delta) #Autocorrelation
m.part = Xupp - Xlow*rho0 - mu0*(1-rho0) #Observation minus the mean of transition distribution
v.part = gamma2*(1-rho0^2) #Variance of transition distribution
loglik = - n*(log(v.part)) - sum(m.part^2/v.part)
-loglik
}
## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations
estimate.OU = function(data, delta, initial.values = NULL){
if(is.null(initial.values)){
n    = length(data)
Xupp = data[2:n]
Xlow = data[1:(n-1)]
mu   = mean(data) ##Starting value for mu0
alpha = -log(sum((Xupp-mu)*(Xlow-mu))/sum((Xlow-mu)^2))/delta ##Starting value for alpha0
s2 = mean(diff(data)^2)/delta ##Quadratic variation, starting value for sigma^2
par.init = c(alpha, mu, s2)   ## Collect starting values
}
if(!is.null(initial.values)) par.init = initial.values
minuslogl = function(alpha,mu,sigma2){
logLik = tryCatch(loglikOU.fun(pars = c(alpha,mu,sigma2), data = data, delta = delta))
if(is.na(logLik))
{
return(10000)
}else
{
return(logLik)
}
}
temp = stats4::mle(minuslogl = minuslogl,
start = list(alpha = par.init[1], mu = par.init[2], sigma2 = par.init[3]))
return(temp)
}
## Function returning 2 x the negative log-likelihood of a trace up to a constant from model
## dXt = - (a*(Xt - m)^2 + lambda) dt + sigma * dWt
## Assuming alpha0, mu0, sigma known
## Strang splitting estimator based on Taylor expansion around mu(lambda)
loglik.fun = function(pars, data, delta, alpha0, mu0, sigma20, pen = 0){
##pars are the parameters to be estimated, tau and a
##data is a trace under linear changing of lambda
##delta is the time step between observations
##alpha0, mu0, sigma20 are parameters already estimated in the OU process from stationary part
tau     = pars[1]             #Ramping time
a       = max(0.1,pars[2])    #Factor in front of (x-m)^2 in drift. Positive
m       = mu0 - alpha0/(2*a)  #Constant mean shift
lambda0 = -alpha0^2/(4*a)     #Stationary level of control parameter
sigma2  = sigma20             #Infinitisimal variance
n       = length(data)
Xupp    = data[2:n]
Xlow    = data[1:(n-1)]
time    = delta*(1:(n-1))
lam.seq    = lambda0*(1-time/tau)
alpha.seq  = 2*sqrt(-a*lam.seq)
gamma2.seq = sigma2/(2*alpha.seq)
rho.seq    = exp(-alpha.seq*delta)
mu.seq     = m + sqrt(-lam.seq/a)
## Calculating the Strang splitting scheme pseudo likelihood
fh.half.tmp = a*delta*(Xlow - mu.seq)/2
fh.half     = (mu.seq*fh.half.tmp+Xlow)/(fh.half.tmp+1)
fh.half.inv = (mu.seq*fh.half.tmp-Xupp)/(fh.half.tmp-1)
mu.h        = fh.half*rho.seq + mu.seq*(1-rho.seq)
m.part      = fh.half.inv - mu.h
var.part    = gamma2.seq*(1-rho.seq^2)
det.Dfh.half.inv = 1/(a*delta*(Xupp-mu.seq)/2-1)^2
loglik      = - sum(log(var.part)) - sum(m.part^2/var.part) +
2*sum(log(det.Dfh.half.inv)) - pen*n*(1/a - 1)*(a < 1)
return(-loglik)
}
## Function returning the estimated parameters
## Input is a data trace, the time step delta between observations, and time passed
## at present since time t_0
estimate.tipping = function(data, delta, initial.values = c(1000,1),
alpha0, mu0, sigma20, pen = pen){
par.init = initial.values
minuslogl = function(pars){
logLik = tryCatch(loglik.fun(pars = pars, data = data, delta = delta,
alpha0 = alpha0, mu0 = mu0, sigma20 = sigma20, pen = pen))
if(is.na(logLik))
{
return(10000)
}else
{
return(logLik)
}
}
temp = optim(par = c(tau = par.init[1], a = par.init[2]),
fn = minuslogl, method = "Nelder-Mead", hessian = FALSE)
return(temp)
}
## Function returning a trajectory up to a crossing time of X(t)
## over the barrier as a function of
## the initial condition X0, the size of the noise sigma,
## the integration time step dt, and the length of the simulation N
## and with time varying lambda:
X.traj <- function(sigma = 0.1, lambda0 = -2, tau = 1000, m = 0, a = 1,
T0 = 0, X0 = sqrt(2), dt = 0.1, Ymax = 1000000){
##T0: Time before ramping starts
##Ymax: Max number of simulated points, if tipping does not happen
Y = 0  ## Counting integration steps up to tipping
xbarrier = m - 2 ## One smaller than crossing point at start
Xtraj = X0
X = X0
## Simulation during stationary period, constant lambda
while(X > xbarrier & Y < T0/dt){
X = S.onestep(sigma = sigma, lambda = lambda0,
m = m, a = a, X0 = X, dt = dt)
Xtraj = c(Xtraj, X)
Y = Y+1
}
## Simulation after lambda starts increasing
while(X > xbarrier & Y < Ymax){
time = dt * (Y - T0/dt) ## Time after lambda starts increasing
lambda = lambda0*(1-time/tau)
X = S.onestep(sigma = sigma, lambda = lambda0*(1 - time/tau),
m = m, a = a, X0 = X, dt = dt)
Xtraj = c(Xtraj, X)
Y = Y + 1
}
Y = Y*dt
return(list(FPT = Y, X = Xtraj))
}
## Function returning a simulation of the simple model one time step
## using the Euler-Maruyama scheme
S.onestep <- function(sigma = 0.1, lambda = 0, m = 0, a = 1, X0 = 1, dt = 0.1){
dWt = rnorm(1,0,1)    ## Draws a random increment
Y   = X0 - a*(X0-m)^2*dt - lambda*dt + sqrt(dt)*sigma*dWt
return(Y)
}
var_fun = function(data){
data = detrend(data, order = 2)
this.var = var(data)
return(this.var)
}
ac1_fun = function(data){
data = detrend(data, order = 2)
#plot(data, type="l")
this.var = var(data)
this.acov = mean(data[2:length(data)]*data[1:(length(data)-1)])
this.ac1 = this.acov/this.var
return(this.ac1)
}
exp_fit_fun = function(t,alpha_slope){
print(alpha_slope)
return(exp(-(abs(alpha_slope*t+4*0.1^2))^0.5))
}
estimate.tipping.linearTrend = function(data, csdindicator, windowsize, jump){
if(csdindicator=="variance_inv")
{
indicator.series = rollapply(data, width = windowsize, FUN = var_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = 1/indicator.series
} else if(csdindicator=="ac1")
{
indicator.series = rollapply(data, width = windowsize, FUN = ac1_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = 1-indicator.series
} else if(csdindicator=="ac1_log")
{
indicator.series = rollapply(data, width = windowsize, FUN = ac1_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = log(indicator.series)
} else if(csdindicator=="gls")
{
indicator.series = rollapply(data, width = windowsize, FUN = phi_gls, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = 1-indicator.series
} else if(csdindicator=="gls_log")
{
indicator.series = rollapply(data, width = windowsize, FUN = phi_gls, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = log(indicator.series)
} else if(csdindicator=="variance_inv_sq")
{
indicator.series = rollapply(data, width = windowsize, FUN = var_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = (1/indicator.series)**2
} else if(csdindicator=="ac1_log_sq")
{
indicator.series = rollapply(data, width = windowsize, FUN = ac1_fun, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = log(indicator.series)**2
if (FALSE){
print(control.parameter.series)
linear.fit.time = 1:length(data)
#linear.fit = nls(formula = indicator.series ~ exp_fit_fun(linear.fit.time,alpha_slope), start=list(alpha_slope=-1/5000))
linear.fit = lm(formula = control.parameter.series ~ linear.fit.time)
tau.linear = -coefficients(linear.fit)[1]/coefficients(linear.fit)[2]
#tau.linear = -4*0.1^2/coefficients(linear.fit)[1]
if (tau.linear<0)
{
tau.linear = 10**6
}
plot(data, type="l")
plot(linear.fit.time, control.parameter.series, sub=csdindicator)
abline(linear.fit)
plot(tau.linear)
}
} else if(csdindicator=="gls_log_sq")
{
indicator.series = rollapply(data, width = windowsize, FUN = phi_gls, partial = FALSE, align = "center", na.pad = TRUE, by = jump)
control.parameter.series = log(indicator.series)**2
}
linear.fit.time = 1:length(data)
linear.fit = lm(formula = control.parameter.series ~ linear.fit.time)
tau.linear = -coefficients(linear.fit)[1]/coefficients(linear.fit)[2]
if (tau.linear<0)
{
tau.linear = 10**6
}
#plot(data, type="l")
#plot(linear.fit.time, control.parameter.series, sub=csdindicator)
#abline(linear.fit)
return(tau.linear)
}
## Take the estimates from the AMOC data
t0 = 1000 ## Knowledge of t0 is presupposed. All synthetic models have the same true value of t0 = 1924.
#alpha0 = 3.06
#mu0 = 0.25
#sigma2 = 0.3/9
#tau = 132.52
#a = 0.87
#m = -1.51
#lambda0 = -2.69
#tc = 2056.52
##############################
## Estimation on each trace ##
##############################
for (filepath in list.files("SamplePathData", full.names = TRUE)){ ## each synthetic model has data stored in a separate .csv file
modelname = str_sub(filepath,16,-5)
print(modelname)
n = 2000
Delta_t = 1
xx = data.matrix(read.csv(filepath))[,-1]
nsim = dim(xx)[2]   ## Number of simulations
xxtime = seq(from = 1, to = n, length.out=n)
xx.data = data.frame(X = c(xx), time = rep(xxtime, nsim),
repetition = rep(1:nsim, each = n))
## xx.data now contains nsim time series of the current examined model
pen = 0
run_penalization_routine = FALSE ## If TRUE the optimal pen as the value with least spread around the tc estimation corresponding to pen = 0
if (run_penalization_routine){
## First with no penalization, determine the estimated tau_r
estim.alpha0 = numeric(nsim)
estim.mu0 = numeric(nsim)
estim.s2 = numeric(nsim)
estim.tau = numeric(nsim)
estim.a = numeric(nsim)
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
estim.alpha0[isim] = coef(temp1)[1]
estim.mu0[isim]   = coef(temp1)[2]
estim.s2[isim]     = coef(temp1)[3]
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.tau[isim] = temp2$par[1]
estim.a[isim]  = temp2$par[2]
}
alpha0_pen0 = median(estim.alpha0)
mu0_pen0 = median(estim.mu0)
s2_pen0 = median(estim.s2)
tau_pen0 = median(estim.tau) ## This value is the fitting target for the subsequent search of an optimal pen
a_pen0 = median(estim.a)
m_pen0 = median(estim.mu0 - estim.alpha0/(2*estim.a))
lambda0_pen0 = median(- estim.alpha0^2/(4*estim.a))
tc_pen0 = median(estim.tau + t0)
#print(paste("alpha0_pen0:",alpha0_pen0))
#print(paste("mu0_pen0:",mu0_pen0))
#print(paste("s2_pen0:",s2_pen0))
#print(paste("tau_pen0:",tau_pen0))
#print(paste("a_pen0:",a_pen0))
#print(paste("m_pen0:",m_pen0))
#print(paste("lambda0_pen0:",lambda0_pen0))
#print(paste("tc_pen0:",tc_pen0))
#Define parameters
crossval_nsim = 50 ## Number of cross validation time series
T0      = t0      ## Time length of stationary dynamics (lambda constant)
sig     = sqrt(s2_pen0) ## Infinitisimal standard deviation: sigma
time    = seq(from = 1, to = n, length.out=n)
lam.seq = lambda0_pen0*(1-pmax(0,(time-t0))/tau_pen0) ## Time varying lambda
alpha.seq = 2*sqrt(-a_pen0*lam.seq) ## Time varying alpha
mu.seq  = m_pen0 + sqrt(-lam.seq/a_pen0) ## Time varying mu
var0    = s2_pen0/(2*alpha0_pen0)    ## Time varying variance
Delta_t = 1                 ## Time step in years (one measurement per month)
nloop   = 20                   ## Number of simulation steps between observation steps
tp      = 1000 - t0            ## Time length of non-stationary dynamics (lambda linearly increasing)
nt      = ceiling(tp/Delta_t) ## Total number of points from t_0 to today
## Generate synthetic time series to find the optimal pen on
crossval_xx = matrix(NA, n, crossval_nsim)
for (isim in 1:crossval_nsim){
#Initial condition is random from stationary distribution
x0 = m_pen0 + sqrt(-lambda0_pen0/a_pen0) + rnorm(1, mean = 0, sd = sig/sqrt(2*alpha0_pen0))
xxx = X.traj(sigma = sig, lambda0 = lambda0_pen0, tau = tau_pen0, m = m_pen0, a = a_pen0,
T0 = T0, X0 = x0, dt = Delta_t/nloop,
Ymax = nloop*n-1)
nx = length(xxx$X) ##If tipping happens before, trajectory is shorter than nt
xxxx = xxx$X[seq(1, nx, nloop)] ##Only keep points at observed times
nxx = length(xxxx)
crossval_xx[1:nxx, isim] = xxxx
}
xxtime <- time[1:n]
crossval_xx.data = data.frame(X = c(crossval_xx), time = rep(xxtime, crossval_nsim),
repetition = rep(1:crossval_nsim, each = n),
model = rep(m_pen0 + sqrt(-lam.seq[1:n]/a_pen0), crossval_nsim))
## End of simulation of crossval_nsim paths
p.vec = seq(0,0.01,0.001) ## Try this range of values for pen. For the AMOC time series in DD23, 0.004 was the optimal value.
MSE = matrix(0,ncol = 2, nrow = length(p.vec))
dimnames(MSE)[[2]] = c("p","MSE")
MSE[,1] = p.vec
for(i in 1:length(p.vec)){ ## For every choice of pen, determine the distribution of estimated tau_r
pen = p.vec[i]
estim.tau = numeric(crossval_nsim) ## Vector to hold estimates
for(isim in 1:crossval_nsim){
xxi   = crossval_xx.data[crossval_xx.data$time > t0 & crossval_xx.data$repetition == isim,"X"]  ##Get data from isim simulation
xxi.0 = crossval_xx.data[crossval_xx.data$time <= t0 & crossval_xx.data$repetition == isim,"X"] ##Get data from isim simulation
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.tau[isim] = temp2$par["tau"]
}
MSE[i,2] = mean((estim.tau-tau_pen0)^2) ## The distributions are assessed in their square error w.r.t. the fitting target at pen = 0
}
## Do full tipping time analysis on all time series with optimal value of penalization p
pen = p.vec[which.min(c(MSE[,2]))] ## pen is chosen as the value where the above square error was minimal
print(paste("Optimal pen:",pen))
}## End of penalization routine
estim.matrix = matrix(0, ncol = 9, nrow = nsim) ## Matrix to hold estimates
dimnames(estim.matrix)[[2]] = c("variance_inv","variance_inv_sq","ac1","ac1_log","ac1_log_sq","gls","gls_log","gls_log_sq","dd23")
for(isim in 1:nsim){
## Get data from isim simulation after time t0, when linear ramping has started
xxi   = xx.data[xx.data$time > t0 & xx.data$repetition == isim,"X"]
xxi   = xxi[xxi>-1.2] ## Remove last data points in case it has tipped
## Get baseline data from isim simulation, before time t0
xxi.0 = xx.data[xx.data$time <= t0 & xx.data$repetition == isim,"X"]
nx = length(xxi)
temp1 = estimate.OU(data = xxi.0, delta = Delta_t)
temp2 = estimate.tipping(data = xxi, delta = Delta_t, initial.values = c(1000, 1),
alpha0 = coef(temp1)[1], mu0 = coef(temp1)[2], sigma20 = coef(temp1)[3], pen = pen)
estim.matrix[isim,"dd23"] = temp2$par[1] + t0
windowsize = 100
jump = 10#50
#estim.matrix[isim,"variance_inv"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "variance_inv", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"variance_inv_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "variance_inv_sq", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"ac1"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"ac1_log"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1_log", windowsize = windowsize, jump = jump) + t0
estim.matrix[isim,"ac1_log_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "ac1_log_sq", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"gls"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls", windowsize = windowsize, jump = jump) + t0
#estim.matrix[isim,"gls_log"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls_log", windowsize = windowsize, jump = jump) + t0
estim.matrix[isim,"gls_log_sq"] = estimate.tipping.linearTrend(data = xxi, csdindicator = "gls_log_sq", windowsize = windowsize, jump = jump) + t0
}
write_xlsx(as.data.frame(estim.matrix), paste("TipEstimResults_",modelname,".xlsx",sep=""))
}
